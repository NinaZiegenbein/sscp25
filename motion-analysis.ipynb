{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86a492e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b96f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_combined_point_cloud(folder, pattern):\n",
    "    folder = Path(folder)\n",
    "    files = list(folder.rglob(pattern))\n",
    "\n",
    "    all_clouds = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file, header=None) # delim_whitespace=True\n",
    "        all_clouds.append(df.to_numpy())\n",
    "\n",
    "    # Ensure all point clouds have the same number of points\n",
    "    lengths = [cloud.shape[0] for cloud in all_clouds]\n",
    "    if len(set(lengths)) != 1:\n",
    "        raise ValueError(f\"Point clouds have different lengths: {lengths}\")\n",
    "    \n",
    "    stacked = np.stack(all_clouds)  # Shape: (N_files, N_points, 3)\n",
    "    avg_cloud = np.mean(stacked, axis=0)  # Shape: (N_points, 3)\n",
    "\n",
    "    return avg_cloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4713f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating ED average point cloud\n",
    "folder_path = \"Aligned_Models\"\n",
    "avg_cloud = average_combined_point_cloud(folder_path, \"*.txt\")\n",
    "np.savetxt(\"avg_points_cloud_ED.txt\", avg_cloud, fmt=\"%.8f\", delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save ES points seperated into endo and epi in same structure as ED\n",
    "# input_folder = \"ES_pointclouds_merged\"\n",
    "# output_folder = \"ES_pointclouds\"\n",
    "\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# for filename in os.listdir(input_folder):\n",
    "#     if filename.startswith(\"MAD_\") and filename.endswith(\"_merged_points.txt\"):\n",
    "#         # Extract participant ID (number after MAD_)\n",
    "#         parts = filename.split(\"_\")\n",
    "#         participant_id = parts[1]\n",
    "\n",
    "#         filepath = os.path.join(input_folder, filename)\n",
    "#         df = pd.read_csv(filepath, header=None, names=[\"x\", \"y\", \"z\"])\n",
    "\n",
    "#         # Split into epi and endo\n",
    "#         df_epi = df.iloc[:801]\n",
    "#         df_endo = df.iloc[801:1602]\n",
    "\n",
    "#         # Create participant folder and save\n",
    "#         participant_folder = os.path.join(output_folder, participant_id)\n",
    "#         os.makedirs(participant_folder, exist_ok=True)\n",
    "#         df_epi.to_csv(os.path.join(participant_folder, \"points_cloud_epi_ES.csv\"), index=False, header=False)\n",
    "#         df_endo.to_csv(os.path.join(participant_folder, \"points_cloud_endo_ES.csv\"), index=False, header=False)\n",
    "\n",
    "#         #print(f\"Processed participant {participant_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc695437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_save_motion(idx, ed_folder, es_folder, output_folder, avg_path):\n",
    "    ed_file = os.path.join(ed_folder, f\"{idx}_merged_points.txt\")\n",
    "    es_file = os.path.join(es_folder, f\"MAD_{idx}_merged_points.txt\")\n",
    "    out_file = os.path.join(output_folder, f\"{idx}_merged_points.txt\")\n",
    "\n",
    "    if not os.path.exists(ed_file) or not os.path.exists(es_file):\n",
    "        print(f\"Skipping {idx}: missing file(s)\")\n",
    "        return\n",
    "\n",
    "    ed = pd.read_csv(ed_file, header=None)\n",
    "    es = pd.read_csv(es_file, header=None)\n",
    "    average_ed = pd.read_csv(avg_path, header=None)\n",
    "    #print(f\"Shapes: ed={ed.shape}, es={es.shape}, average_ed={average_ed.shape}\")\n",
    "\n",
    "    if ed.shape != es.shape or ed.shape != average_ed.shape:\n",
    "        \n",
    "        print(f\"Skipping {idx}: shape mismatch\")\n",
    "        return\n",
    "\n",
    "    motion = ed - es + average_ed\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    motion.to_csv(out_file, sep=',', index=False, header=False)\n",
    "    print(f\"Saved: {idx}_merged_points.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c71514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: 170_merged_points.txt\n",
      "Saved: 188_merged_points.txt\n",
      "Saved: 158_merged_points.txt\n",
      "Saved: 144_merged_points.txt\n",
      "Saved: 12_merged_points.txt\n",
      "Saved: 95_merged_points.txt\n",
      "Saved: 175_merged_points.txt\n",
      "Saved: 141_merged_points.txt\n",
      "Saved: 17_merged_points.txt\n",
      "Saved: 127_merged_points.txt\n",
      "Saved: 169_merged_points.txt\n",
      "Saved: 171_merged_points.txt\n",
      "Saved: 91_merged_points.txt\n",
      "Saved: 27_merged_points.txt\n",
      "Saved: 145_merged_points.txt\n",
      "Saved: 75_merged_points.txt\n",
      "Saved: 123_merged_points.txt\n",
      "Saved: 94_merged_points.txt\n",
      "Saved: 112_merged_points.txt\n",
      "Saved: 190_merged_points.txt\n",
      "Skipping 70: missing file(s)\n",
      "Saved: 168_merged_points.txt\n",
      "Saved: 58_merged_points.txt\n",
      "Saved: 126_merged_points.txt\n",
      "Saved: 92_merged_points.txt\n",
      "Saved: 146_merged_points.txt\n",
      "Saved: 76_merged_points.txt\n",
      "Skipping 108: missing file(s)\n",
      "Saved: 120_merged_points.txt\n",
      "Saved: 97_merged_points.txt\n",
      "Saved: 139_merged_points.txt\n",
      "Saved: 21_merged_points.txt\n",
      "Saved: 73_merged_points.txt\n",
      "Saved: 15_merged_points.txt\n",
      "Saved: 8_merged_points.txt\n",
      "Saved: 93_merged_points.txt\n",
      "Saved: 39_merged_points.txt\n",
      "Saved: 109_merged_points.txt\n",
      "Saved: 210_merged_points.txt\n",
      "Saved: 121_merged_points.txt\n",
      "Saved: 11_merged_points.txt\n",
      "Saved: 96_merged_points.txt\n",
      "Saved: 138_merged_points.txt\n",
      "Saved: 142_merged_points.txt\n",
      "Saved: 309_merged_points.txt\n",
      "Saved: 67_merged_points.txt\n",
      "Saved: 157_merged_points.txt\n",
      "Saved: 4_merged_points.txt\n",
      "Saved: 163_merged_points.txt\n",
      "Saved: 53_merged_points.txt\n",
      "Saved: 35_merged_points.txt\n",
      "Saved: 105_merged_points.txt\n",
      "Saved: 182_merged_points.txt\n",
      "Saved: 205_merged_points.txt\n",
      "Saved: 152_merged_points.txt\n",
      "Saved: 62_merged_points.txt\n",
      "Saved: 1_merged_points.txt\n",
      "Saved: 166_merged_points.txt\n",
      "Saved: 118_merged_points.txt\n",
      "Saved: 130_merged_points.txt\n",
      "Saved: 162_merged_points.txt\n",
      "Saved: 204_merged_points.txt\n",
      "Saved: 63_merged_points.txt\n",
      "Saved: 304_merged_points.txt\n",
      "Saved: 129_merged_points.txt\n",
      "Saved: 167_merged_points.txt\n",
      "Saved: 218_merged_points.txt\n",
      "Saved: 31_merged_points.txt\n",
      "Saved: 65_merged_points.txt\n",
      "Saved: 202_merged_points.txt\n",
      "Skipping 302: missing file(s)\n",
      "Saved: 107_merged_points.txt\n",
      "Saved: 149_merged_points.txt\n",
      "Saved: 180_merged_points.txt\n",
      "Saved: 150_merged_points.txt\n",
      "Saved: 178_merged_points.txt\n",
      "Saved: 3_merged_points.txt\n",
      "Saved: 84_merged_points.txt\n",
      "Saved: 164_merged_points.txt\n",
      "Saved: 54_merged_points.txt\n",
      "Skipping 102: missing file(s)\n",
      "Saved: 132_merged_points.txt\n",
      "Saved: 160_merged_points.txt\n",
      "Saved: 198_merged_points.txt\n",
      "Saved: 78_merged_points.txt\n",
      "Saved: 106_merged_points.txt\n",
      "Saved: 181_merged_points.txt\n",
      "Saved: 151_merged_points.txt\n",
      "Saved: 137_merged_points.txt\n",
      "Saved: 103_merged_points.txt\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "ed_folder = \"Aligned_Models\"\n",
    "es_folder = \"ES_pointclouds_merged\"\n",
    "output_folder = \"pc_motion\"\n",
    "avg_path = \"avg_points_cloud_ED.txt\"\n",
    "\n",
    "# Loop through all ED files\n",
    "for fname in os.listdir(ed_folder):\n",
    "    if fname.endswith(\"_merged_points.txt\"):\n",
    "        try:\n",
    "            idx = fname.split(\"_\")[0]  # gets the number before '_merged_points.txt'\n",
    "            compute_and_save_motion(idx, ed_folder, es_folder, output_folder, avg_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed on {fname}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b943390",
   "metadata": {},
   "source": [
    "# How to run without overwriting results\n",
    "* save pc_motion folder outside of this path and rename to Aligned_Models\n",
    "* copy the results folder, since saxomode-pca takes participant number from results folder\n",
    "* then call saxomode-pca\n",
    "* resulting scores are in PCA_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c51f0c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
