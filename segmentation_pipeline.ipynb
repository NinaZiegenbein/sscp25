{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "244138ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import pydicom\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import zoom\n",
    "from monai.bundle import load_bundle_config\n",
    "from huggingface_hub import hf_hub_download\n",
    "import cv2 #The import-call for cv2 is \"pip install opencv-python\" (not cv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da478a5",
   "metadata": {},
   "source": [
    "### Segmentation of DICOM files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ee1b5",
   "metadata": {},
   "source": [
    "Must know:\n",
    "- ID 207: Add a folder named \"sax\" in cine and move all subfolders (with the weird names) in there for consistency\n",
    "\n",
    "Good to know:\n",
    "- All missing IDs have a \"-\" in column \"Folders (y/n)\". So when you find out folder-order and first and last for the missing ones, change that to \"y\" and run the code again. \n",
    "- We should ask about 187 (no sax folder), for now it also has a \"-\" and is ignored.\n",
    "- Otherwise scroll down for the function calls. I structured it this way so it's easy to make into a .py file, but notebook is easier for debugging etc.\n",
    "- Check out the TODO tag below about the affine to convert it into Nifti format and wether that is necessary for input into Giulia's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1deb5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "    Extracts sliceloc and triggertime values from a filename.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): Filename containing 'sliceloc_{val}_triggertime_{val}'.\n",
    "\n",
    "    Returns:\n",
    "        tuple[float | None, float | None]: Parsed sliceloc and triggertime as floats, or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"sliceloc_([-\\d.]+)_triggertime_([-\\d.]+)\", filename)\n",
    "    if match:\n",
    "        return float(match.group(1)), float(match.group(2))\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773df45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_files_n(df, base_path):\n",
    "    \"\"\"\n",
    "    Selects one relevant file per slice location for each patient based on ED frame and apex–base range.\n",
    "\n",
    "    For each patient, searches {base_path}/{ID}/cine/sax/ (recursively) for files named \n",
    "    like '...sliceloc_{val}_triggertime_{val}'. Keeps only slices within the apex–base \n",
    "    range and selects the earliest (ED Slice == 0) or latest frame per slice.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): df_y (see above); DataFrame of \"ED_slices_and_timepoints.csv\", without series-substructure\n",
    "        base_path (str): Root path containing the patient folders.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: Mapping from patient ID to selected file paths.\n",
    "    \"\"\"\n",
    "    patient_files = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        pid = str(row[\"ID\"]).strip()\n",
    "        try:\n",
    "            ed_slice = int(row[\"ED frame\"])\n",
    "            apex = float(row[\"apex\"])\n",
    "            base = float(row[\"base\"])\n",
    "        except (ValueError, TypeError):\n",
    "            # Skip malformed rows\n",
    "            continue\n",
    "\n",
    "        folder = os.path.join(base_path, pid, \"cine\", \"sax\")\n",
    "        if not os.path.isdir(folder):\n",
    "            print(f\"Warning: folder not found for patient {pid}\")\n",
    "            continue\n",
    "\n",
    "        files = [p for p in Path(folder).rglob(\"*\") if p.is_file()]\n",
    "        parsed = []\n",
    "\n",
    "        # Parse filenames\n",
    "        for f in files:\n",
    "            fname = f.name\n",
    "            sliceloc, triggertime = parse_filename(fname)\n",
    "            if sliceloc is not None and triggertime is not None:\n",
    "                parsed.append((f, sliceloc, triggertime))\n",
    "\n",
    "        if not parsed:\n",
    "            print(f\"Warning: no valid files for patient {pid}\")\n",
    "            continue\n",
    "\n",
    "        # Group by sliceloc → list of triggertimes\n",
    "        sliceloc_map = defaultdict(list)\n",
    "        for f, sliceloc, triggertime in parsed:\n",
    "            sliceloc_map[sliceloc].append((f, triggertime))\n",
    "\n",
    "        lower, upper = sorted([apex, base])\n",
    "        selected = []\n",
    "\n",
    "        for sliceloc, items in sliceloc_map.items():\n",
    "            if lower <= sliceloc <= upper:\n",
    "                times = [tt for _, tt in items]\n",
    "                if ed_slice == 0:\n",
    "                    target_tt = min(times)\n",
    "                else:\n",
    "                    target_tt = max(times)\n",
    "\n",
    "                # Add the file with this sliceloc + target triggertime\n",
    "                for f, tt in items:\n",
    "                    if tt == target_tt:\n",
    "                        selected.append((sliceloc, str(f)))\n",
    "                        break  # Only one per sliceloc\n",
    "        \n",
    "        # Sort by sliceloc before storing\n",
    "        selected.sort(key=lambda x: x[0])\n",
    "        patient_files[pid] = [f for _, f in selected]\n",
    "\n",
    "        \n",
    "\n",
    "    return patient_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6942e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_files_y(df, base_path):\n",
    "    \"\"\"\n",
    "    Selects one relevant file per folder-defined slice for each patient using ED frame.\n",
    "\n",
    "    For each patient, looks inside {base_path}/{ID}/cine/sax/series_{folder}/ for files.\n",
    "    The slice locations are inferred from subfolder names (e.g. 'series_25').\n",
    "    The column 'folder order' lists all available series (in order),\n",
    "    while 'apex' and 'base' define the first and last folder to include.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with columns 'ID', 'ED Slice', 'apex', 'base', and 'folder order'.\n",
    "        base_path (str): Root path containing the patient folders.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: Mapping from patient ID to selected file paths.\n",
    "    \"\"\"\n",
    "\n",
    "    patient_files = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        pid = str(row[\"ID\"]).strip()\n",
    "        try:\n",
    "            ed_slice = int(row[\"ED frame\"])\n",
    "            apex = int(row[\"apex\"])\n",
    "            base = int(row[\"base\"])\n",
    "            folder_order = str(row[\"folder order\"]).strip()\n",
    "        except (ValueError, TypeError):\n",
    "            print(f\"Could not extract values for ID {pid}\")\n",
    "            continue\n",
    "\n",
    "        if not folder_order or folder_order.lower() == \"nan\":\n",
    "            continue\n",
    "\n",
    "        sax_root = os.path.join(base_path, pid, \"cine\", \"sax\")\n",
    "        if not os.path.isdir(sax_root):\n",
    "            print(f\"Warning: folder not found for patient {pid}\")\n",
    "            continue\n",
    "\n",
    "        # Get ordered folder list (as ints)\n",
    "        order = [int(x) for x in folder_order.split(\"-\") if x.isdigit()]\n",
    "        lower_idx, upper_idx = order.index(apex), order.index(base)\n",
    "        selected_series = order[lower_idx:upper_idx+1]\n",
    "\n",
    "        selected = []\n",
    "\n",
    "        for sliceloc in selected_series:\n",
    "            series_path = os.path.join(sax_root, f\"series_{sliceloc}\")\n",
    "            if not os.path.isdir(series_path):\n",
    "                print(f\"Warning: missing folder series_{sliceloc} for patient {pid}\")\n",
    "                continue\n",
    "\n",
    "            files = glob.glob(os.path.join(series_path, \"*\"))\n",
    "            triggertimes = []\n",
    "\n",
    "            for f in files:\n",
    "                _, tt = parse_filename(os.path.basename(f))\n",
    "                if tt is not None:\n",
    "                    triggertimes.append((f, tt))\n",
    "\n",
    "            if not triggertimes:\n",
    "                continue\n",
    "\n",
    "            if ed_slice == 0:\n",
    "                chosen_file = min(triggertimes, key=lambda x: x[1])[0]\n",
    "            else:\n",
    "                chosen_file = max(triggertimes, key=lambda x: x[1])[0]\n",
    "\n",
    "            selected.append(chosen_file)\n",
    "\n",
    "        patient_files[pid] = selected\n",
    "\n",
    "    return patient_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc20dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_excluded_size(f, pid=None):\n",
    "    \"\"\"\n",
    "    Helper Function: Determines whether a file should be excluded based on its size.\n",
    "    Applies a default size exclusion range (approx. 168 KB), but allows \n",
    "    per-patient overrides for specific cases where the real series has \n",
    "    a different file size.\n",
    "\n",
    "    Parameters:\n",
    "        f (Path): Path object pointing to the DICOM file.\n",
    "        pid (str or int, optional): Patient ID used to apply override exclusion rules.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file should be excluded based on its size, False otherwise.\n",
    "    \"\"\"\n",
    "    # Default range: files that show as 168 KB\n",
    "    default_range = (167000, 169000)\n",
    "\n",
    "    # Manual overrides for specific PIDs\n",
    "    overrides = {\n",
    "        \"4\": (102000, 104000),     # PID 4 → exclude ~103 KB\n",
    "        \"169\": (132000, 134000),   # PID 169 → exclude ~133 KB\n",
    "    }\n",
    "\n",
    "    lower, upper = overrides.get(str(pid), default_range)\n",
    "\n",
    "    try:\n",
    "        size = f.stat().st_size\n",
    "        return lower <= size <= upper\n",
    "    except FileNotFoundError:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4615e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_files_new(df, base_path):\n",
    "    \"\"\"\n",
    "    Selects the ED frame DICOM file for each slice between apex and base for each patient.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with columns ['ID', 'ED frame', 'apex', 'base']\n",
    "        base_path (str): Root directory containing patient folders (e.g., 'MAD_OUS_sorted')\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: Mapping from patient ID to selected file paths.\n",
    "    \"\"\"\n",
    "    patient_files = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            pid = str(row[\"ID\"]).strip()\n",
    "            ed_frame = int(row[\"ED frame\"])\n",
    "            apex = int(row[\"apex\"])\n",
    "            base = int(row[\"base\"])\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"There was an error in reading the csv file.\")\n",
    "            continue \n",
    "\n",
    "        lower, upper = sorted([apex, base])\n",
    "        selected = []\n",
    "\n",
    "        for slice_num in range(lower, upper + 1):\n",
    "            slice_folder = Path(base_path) / f\"MAD_{pid}\" / \"Study01\" / \"cine\" / \"SAX\" / f\"slice_{slice_num}\"\n",
    "            if not slice_folder.exists():\n",
    "                print(f\"Folder slice_{slice_num} for patient {pid} can not be found and is skipped.\")\n",
    "                continue\n",
    "\n",
    "            files = sorted(slice_folder.glob(\"frame*.dcm\"))\n",
    "            if not files:\n",
    "                print(f\"Folder slice_{slice_num} for patient {pid} does not contain any frame*.dcm files.\")\n",
    "                continue\n",
    "\n",
    "            # Filter out files of unwanted size\n",
    "            valid_files = [f for f in files if not is_excluded_size(f, pid)]\n",
    "            #print(valid_files[ed_frame].stat().st_size)\n",
    "\n",
    "            if not valid_files:\n",
    "                print(f\"skipping patient {pid} and slice {slice_num} because all files are not valid (low quality added series)\")\n",
    "                continue  # all were 168 KB — skip this slice silently\n",
    "\n",
    "            # Use reindexed list to get ED frame\n",
    "            if ed_frame < len(valid_files):\n",
    "                selected.append(str(valid_files[ed_frame]))\n",
    "            else:\n",
    "                print(f\"ED_Frame {ed_frame} number larger than number of files in folder, Patient {pid}, Slice {slice_num} skipped\")\n",
    "\n",
    "            \n",
    "        patient_files[pid] = selected\n",
    "\n",
    "    return patient_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "597f4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(img, target_shape):\n",
    "    \"\"\"\n",
    "    Resize a 2D image to the target shape using OpenCV.\n",
    "\n",
    "    Parameters:\n",
    "        img (np.ndarray): Input image to resize.\n",
    "        target_shape (tuple[int, int]): Desired output shape (height, width).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Resized image.\n",
    "    \"\"\"\n",
    "    # Add padding if one of the image dimensions are smaller than the target shape\n",
    "    if img.shape[0] < target_shape[0] or img.shape[1] < target_shape[1]:\n",
    "\n",
    "        h, w = img.shape\n",
    "\n",
    "        if h < target_shape[0]:\n",
    "            pad_top = (target_shape[0] - h) // 2\n",
    "            pad_bottom = target_shape[0] - h - pad_top\n",
    "        else:\n",
    "            pad_top = 0\n",
    "            pad_bottom = 0\n",
    "\n",
    "\n",
    "        if w < target_shape[1]:\n",
    "            pad_left = (target_shape[1] - w) // 2\n",
    "            pad_right = target_shape[1] - w - pad_left\n",
    "        else:\n",
    "            pad_left = 0\n",
    "            pad_right = 0\n",
    "\n",
    "\n",
    "        img = cv2.copyMakeBorder(\n",
    "                        img,\n",
    "                        top=pad_top,\n",
    "                        bottom=pad_bottom,\n",
    "                        left=pad_left,\n",
    "                        right=pad_right,\n",
    "                        borderType=cv2.BORDER_CONSTANT,\n",
    "                        value=0  # or use the image mean\n",
    "                    )\n",
    "    \n",
    "    # Resize the image if it exceeds the target shape\n",
    "    if img.shape[0] > target_shape[0] or img.shape[1] > target_shape[1]:\n",
    "        img = cv2.resize(img, (target_shape[1], target_shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "299d3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_segmentation(files_dicts, output_root, save_as_stack: bool):\n",
    "    \"\"\"\n",
    "    Runs MONAI ventricular segmentation on all DICOM files provided in files_dicts.\n",
    "\n",
    "    Parameters:\n",
    "        files_dicts (list[dict]): List of dicts (e.g., [files_n, files_y]) with {pid: [file_paths]}.\n",
    "        output_root (str): Root folder where output NIfTI files will be saved.\n",
    "        save_as_stack (bool): If True, saves all slices of a patient as a single NIfTI stack.\n",
    "    \"\"\"\n",
    "    # Load MONAI network config & weights\n",
    "    parser = load_bundle_config(\"MONAI\", \"train.json\")\n",
    "    net = parser.get_parsed_content(\"network_def\")\n",
    "\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=\"MONAI/ventricular_short_axis_3label\",\n",
    "        filename=\"models/model.pt\"\n",
    "    )\n",
    "    net.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    net.eval()\n",
    "\n",
    "    target_shape = (256, 256) \n",
    "\n",
    "    for files_dict in files_dicts:\n",
    "        for pid, paths in files_dict.items():\n",
    "            num_slices = len(paths)\n",
    "            img_stack = []\n",
    "            seg_stack = []\n",
    "            for idx, path in enumerate(paths):\n",
    "                # Read and preprocess DICOM\n",
    "                ds = pydicom.dcmread(path)\n",
    "                img = ds.pixel_array.astype(np.float32)\n",
    "\n",
    "                im_resized = resize_image(img, target_shape)\n",
    "\n",
    "                # Normalize and add batch & channel dims\n",
    "                normed_im = im_resized / im_resized.max()\n",
    "                input_tensor = torch.from_numpy(normed_im).float()[None, None, :, :]\n",
    "\n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    pred = net(input_tensor)\n",
    "                    pred = torch.softmax(pred[0], dim=0)\n",
    "                    seg = torch.argmax(pred, dim=0).numpy()\n",
    "\n",
    "                if save_as_stack:\n",
    "                    img_stack.append(im_resized)\n",
    "                    seg_stack.append(seg)\n",
    "                else:\n",
    "                    # Save\n",
    "                    pid_folder = os.path.join(output_root, str(pid))\n",
    "                    os.makedirs(pid_folder, exist_ok=True)\n",
    "\n",
    "                    affine = np.eye(4)  # identity affine \n",
    "                    #TODO: Look into this. This is an identity affine to map from numpy array to nifti file format, but we should probably\n",
    "                    # use the one from the DICOM - or does this not matter for input into Giulia's code?\n",
    "\n",
    "                    nib.save(nib.Nifti1Image(im_resized, affine), os.path.join(pid_folder, f\"{idx}_img.nii.gz\"))\n",
    "                    nib.save(nib.Nifti1Image(seg.astype(np.uint8), affine), os.path.join(pid_folder, f\"{idx}_seg.nii.gz\"))\n",
    "\n",
    "                    print(f\"Saved {pid} slice {idx}\")\n",
    "        \n",
    "            if save_as_stack and num_slices > 0: # NOTE: The order of the slices for patients with no folder structure is not necessarily correct.\n",
    "                \n",
    "                # Save the entire stack as a single NIfTI file\n",
    "                pid_folder = os.path.join(output_root, str(pid))\n",
    "                os.makedirs(pid_folder, exist_ok=True)\n",
    "\n",
    "                img_stack = np.stack(img_stack, axis=-1)\n",
    "                seg_stack = np.stack(seg_stack, axis=-1)\n",
    "\n",
    "                affine = np.eye(4)  # identity affine for the stack\n",
    "\n",
    "                nib.save(nib.Nifti1Image(img_stack, affine), os.path.join(pid_folder, \"img_stack.nii.gz\"))\n",
    "                nib.save(nib.Nifti1Image(seg_stack.astype(np.uint8), affine), os.path.join(pid_folder, \"seg_stack.nii.gz\"))\n",
    "\n",
    "                print(f\"ID {pid}: Saved image and segmentation stacks\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73109e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping patient 3 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 4 and slice 6 because all files are not valid (low quality added series)\n",
      "ED_Frame 57 number larger than number of files in folder, Patient 4, Slice 7 skipped\n",
      "skipping patient 8 and slice 8 because all files are not valid (low quality added series)\n",
      "skipping patient 17 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 21 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 39 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 53 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 54 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 58 and slice 6 because all files are not valid (low quality added series)\n",
      "skipping patient 63 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 65 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 70 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 73 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 75 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 91 and slice 8 because all files are not valid (low quality added series)\n",
      "skipping patient 91 and slice 8 because all files are not valid (low quality added series)\n",
      "skipping patient 93 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 94 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 97 and slice 8 because all files are not valid (low quality added series)\n",
      "skipping patient 103 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 105 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 106 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 107 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 109 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 112 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 118 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 123 and slice 7 because all files are not valid (low quality added series)\n",
      "ED_Frame 38 number larger than number of files in folder, Patient 127, Slice 6 skipped\n",
      "skipping patient 129 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 130 and slice 8 because all files are not valid (low quality added series)\n",
      "skipping patient 132 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 137 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 139 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 141 and slice 6 because all files are not valid (low quality added series)\n",
      "skipping patient 144 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 145 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 149 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 152 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 157 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 160 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 162 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 164 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 167 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 169 and slice 6 because all files are not valid (low quality added series)\n",
      "skipping patient 170 and slice 7 because all files are not valid (low quality added series)\n",
      "skipping patient 188 and slice 7 because all files are not valid (low quality added series)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/monai/bundle/reference_resolver.py:226: UserWarning: Detected deprecated name 'optional_packages_version' in configuration file, replacing with 'required_packages_version'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 15: Saved image and segmentation stacks\n",
      "ID 114: Saved image and segmentation stacks\n",
      "ID 126: Saved image and segmentation stacks\n",
      "ID 130: Saved image and segmentation stacks\n",
      "ID 138: Saved image and segmentation stacks\n",
      "ID 163: Saved image and segmentation stacks\n",
      "ID 207: Saved image and segmentation stacks\n",
      "ID 229: Saved image and segmentation stacks\n",
      "ID 304: Saved image and segmentation stacks\n",
      "ID 11: Saved image and segmentation stacks\n",
      "ID 168: Saved image and segmentation stacks\n",
      "ID 173: Saved image and segmentation stacks\n",
      "ID 190: Saved image and segmentation stacks\n",
      "ID 195: Saved image and segmentation stacks\n",
      "ID 198: Saved image and segmentation stacks\n",
      "ID 202: Saved image and segmentation stacks\n",
      "ID 203: Saved image and segmentation stacks\n",
      "ID 204: Saved image and segmentation stacks\n",
      "ID 205: Saved image and segmentation stacks\n",
      "ID 210: Saved image and segmentation stacks\n",
      "ID 218: Saved image and segmentation stacks\n",
      "ID 232: Saved image and segmentation stacks\n",
      "ID 302: Saved image and segmentation stacks\n",
      "ID 309: Saved image and segmentation stacks\n",
      "ID 1: Saved image and segmentation stacks\n",
      "ID 3: Saved image and segmentation stacks\n",
      "ID 4: Saved image and segmentation stacks\n",
      "ID 8: Saved image and segmentation stacks\n",
      "ID 12: Saved image and segmentation stacks\n",
      "ID 17: Saved image and segmentation stacks\n",
      "ID 21: Saved image and segmentation stacks\n",
      "ID 27: Saved image and segmentation stacks\n",
      "ID 31: Saved image and segmentation stacks\n",
      "ID 35: Saved image and segmentation stacks\n",
      "ID 39: Saved image and segmentation stacks\n",
      "ID 53: Saved image and segmentation stacks\n",
      "ID 54: Saved image and segmentation stacks\n",
      "ID 58: Saved image and segmentation stacks\n",
      "ID 62: Saved image and segmentation stacks\n",
      "ID 63: Saved image and segmentation stacks\n",
      "ID 65: Saved image and segmentation stacks\n",
      "ID 67: Saved image and segmentation stacks\n",
      "ID 70: Saved image and segmentation stacks\n",
      "ID 73: Saved image and segmentation stacks\n",
      "ID 75: Saved image and segmentation stacks\n",
      "ID 76: Saved image and segmentation stacks\n",
      "ID 78: Saved image and segmentation stacks\n",
      "ID 84: Saved image and segmentation stacks\n",
      "ID 91: Saved image and segmentation stacks\n",
      "ID 92: Saved image and segmentation stacks\n",
      "ID 93: Saved image and segmentation stacks\n",
      "ID 94: Saved image and segmentation stacks\n",
      "ID 95: Saved image and segmentation stacks\n",
      "ID 96: Saved image and segmentation stacks\n",
      "ID 97: Saved image and segmentation stacks\n",
      "ID 102: Saved image and segmentation stacks\n",
      "ID 103: Saved image and segmentation stacks\n",
      "ID 105: Saved image and segmentation stacks\n",
      "ID 106: Saved image and segmentation stacks\n",
      "ID 107: Saved image and segmentation stacks\n",
      "ID 108: Saved image and segmentation stacks\n",
      "ID 109: Saved image and segmentation stacks\n",
      "ID 112: Saved image and segmentation stacks\n",
      "ID 118: Saved image and segmentation stacks\n",
      "ID 120: Saved image and segmentation stacks\n",
      "ID 121: Saved image and segmentation stacks\n",
      "ID 123: Saved image and segmentation stacks\n",
      "ID 126: Saved image and segmentation stacks\n",
      "ID 127: Saved image and segmentation stacks\n",
      "ID 129: Saved image and segmentation stacks\n",
      "ID 130: Saved image and segmentation stacks\n",
      "ID 132: Saved image and segmentation stacks\n",
      "ID 137: Saved image and segmentation stacks\n",
      "ID 138: Saved image and segmentation stacks\n",
      "ID 139: Saved image and segmentation stacks\n",
      "ID 141: Saved image and segmentation stacks\n",
      "ID 142: Saved image and segmentation stacks\n",
      "ID 144: Saved image and segmentation stacks\n",
      "ID 145: Saved image and segmentation stacks\n",
      "ID 146: Saved image and segmentation stacks\n",
      "ID 149: Saved image and segmentation stacks\n",
      "ID 150: Saved image and segmentation stacks\n",
      "ID 151: Saved image and segmentation stacks\n",
      "ID 152: Saved image and segmentation stacks\n",
      "ID 157: Saved image and segmentation stacks\n",
      "ID 158: Saved image and segmentation stacks\n",
      "ID 160: Saved image and segmentation stacks\n",
      "ID 161: Saved image and segmentation stacks\n",
      "ID 162: Saved image and segmentation stacks\n",
      "ID 164: Saved image and segmentation stacks\n",
      "ID 166: Saved image and segmentation stacks\n",
      "ID 167: Saved image and segmentation stacks\n",
      "ID 169: Saved image and segmentation stacks\n",
      "ID 170: Saved image and segmentation stacks\n",
      "ID 171: Saved image and segmentation stacks\n",
      "ID 175: Saved image and segmentation stacks\n",
      "ID 176: Saved image and segmentation stacks\n",
      "ID 178: Saved image and segmentation stacks\n",
      "ID 180: Saved image and segmentation stacks\n",
      "ID 181: Saved image and segmentation stacks\n",
      "ID 182: Saved image and segmentation stacks\n",
      "ID 188: Saved image and segmentation stacks\n"
     ]
    }
   ],
   "source": [
    "# This would be main in .py\n",
    "\n",
    "# read in csv split on folders y/n\n",
    "csv_file = \"ED_slices_and_timepoints.csv\" #For the future, once we structure our folders/files better we need to (probably) adjust this import\n",
    "csv_file_new = \"newDataSegmentation.csv\" \n",
    "df = pd.read_csv(csv_file)\n",
    "df_new = pd.read_csv(csv_file_new)\n",
    "#display(df)\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "df_new.columns = df_new.columns.str.strip()\n",
    "df[\"Folders (y/n)\"] = df[\"Folders (y/n)\"].str.strip().str.lower()\n",
    "\n",
    "df_y = df[df[\"Folders (y/n)\"] == 'y'].reset_index(drop=True)\n",
    "df_n = df[df[\"Folders (y/n)\"] == 'n'].reset_index(drop=True)\n",
    "\n",
    "#display(df_n)\n",
    "#display(df_y)\n",
    "\n",
    "# Change this based on where you store the data\n",
    "#base_path = \"/Users/inad001/Documents/SSCP25/Data and scripts SSCP25/CMR_image_data/new data-dicom\" #\"/Users/inad001/Documents/SSCP25/Data and scripts SSCP25/CMR_image_data/new data-dicom\"\n",
    "base_path = \"/Users/au698484/Documents/SSCP25_data/Data and scripts SSCP25/CMR_image_data/new data-dicom\"\n",
    "base_path_new = \"/Users/au698484/Documents/SSCP25_data/newData/MAD_OUS_sorted\"\n",
    "\n",
    "\n",
    "files_n = get_relevant_files_n(df_n, base_path)\n",
    "files_y = get_relevant_files_y(df_y, base_path)\n",
    "files_new = get_relevant_files_new(df_new, base_path_new)\n",
    "\n",
    "# Call segmentation and save segmentations and images under specified output file (change to match your own destination)\n",
    "run_segmentation([files_n, files_y, files_new], output_root=\"/Users/au698484/Documents/SSCP25_data/segmented_data\", save_as_stack=True) #\"/Users/inad001/Documents/SSCP25/segmented_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e549c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_n.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e87a9f1",
   "metadata": {},
   "source": [
    "### Create h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from myo_cavity import create_h5_file\n",
    "\n",
    "patients = list(files_n.keys()) + list(files_y.keys())\n",
    "segmentation_base_path = \"/Users/inad001/Documents/SSCP25/segmented_data\"\n",
    "ouput_path = \"seg_files/\"\n",
    "\n",
    "create_h5_file(patients=patients, base_path=segmentation_base_path, output_dir=ouput_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Open the file in read mode\n",
    "with h5py.File('seg_files/11.h5', 'r') as file:\n",
    "    # Print all root-level groups/datasets\n",
    "    def print_structure(name, obj):\n",
    "        print(name)\n",
    "\n",
    "    file.visititems(print_structure)\n",
    "\n",
    "    # Access a specific dataset (example)\n",
    "    dataset = file['resolution']\n",
    "    data = dataset[:]  # Convert to a NumPy array\n",
    "    print(data)\n",
    "    \n",
    "    # Print unique values in the dataset\n",
    "    unique_values = np.unique(data)\n",
    "    print(\"Unique values in LVmask:\", unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"seg_files/130.h5\", 'r') as h5_file:\n",
    "    segg = h5_file['RV_mask'][:]\n",
    "    imres = h5_file['resolution'][:]\n",
    "num_slices = segg.shape[0]  # Assuming slices are along the third dimension\n",
    "grid_size = int(np.ceil(np.sqrt(num_slices))) # Determine grid size\n",
    "\n",
    "fig, axes = plt.subplots(grid_size, grid_size, figsize=(8, 8))\n",
    "\n",
    "for i in range(num_slices):\n",
    "    row = i // grid_size\n",
    "    col = i % grid_size\n",
    "    label_1_slice = segg[i, :, :]\n",
    "    axes[row, col].imshow(label_1_slice, cmap='gray')\n",
    "    axes[row, col].set_title(f\"Slice {i+1}\")\n",
    "    axes[row, col].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41a0e40",
   "metadata": {},
   "source": [
    "### Segmentation of NIfTI files\n",
    "\n",
    "This part collects CMR NIfTI files for the patients located in the ED_segmentation_data folder and predicts a segmentation for each patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmr_nifti_files(base_path):\n",
    "    \"\"\"\n",
    "    Gets all CMR NIfTI files for all patients in the specified base path.\n",
    "\n",
    "    Parameters:\n",
    "        base_path (str): Root path containing the patient folders.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: Mapping from patient ID to lists of file paths.\n",
    "    \"\"\"\n",
    "    patient_files = {}\n",
    "\n",
    "    for patient in os.listdir(base_path):\n",
    "        pid = patient.strip()\n",
    "\n",
    "        patient_path = os.path.join(base_path, pid)\n",
    "\n",
    "        if not os.path.isdir(patient_path):\n",
    "            print(f\"Warning: {patient_path} is not a directory\")\n",
    "            continue\n",
    "\n",
    "        files = [os.path.join(patient_path, f) for f in os.listdir(patient_path) if f.startswith(\"cmr\") and f.endswith(\".nii\")]\n",
    "\n",
    "        if not files:\n",
    "            print(f\"Warning: no valid files found for patient {pid}\")\n",
    "            continue\n",
    "\n",
    "        patient_files[pid] = files\n",
    "    \n",
    "    return patient_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8051de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_segmentation_nifti(files_dicts, output_root, save_as_stack: bool):\n",
    "    \"\"\"\n",
    "    Runs MONAI ventricular segmentation on all CMR NIfTI files provided in files_dicts.\n",
    "\n",
    "    Parameters:\n",
    "        files_dicts (list[dict]): List of dicts (e.g., [files_n, files_y]) with {pid: [file_paths]}.\n",
    "        output_root (str): Root folder where output NIfTI files will be saved.\n",
    "        save_as_stack (bool): If True, saves all slices of a patient as a single NIfTI stack.\n",
    "    \"\"\"\n",
    "    # Load MONAI network config & weights\n",
    "    parser = load_bundle_config(\"MONAI\", \"train.json\")\n",
    "    net = parser.get_parsed_content(\"network_def\")\n",
    "\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=\"MONAI/ventricular_short_axis_3label\",\n",
    "        filename=\"models/model.pt\"\n",
    "    )\n",
    "    net.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    net.eval()\n",
    "\n",
    "    target_shape = (256, 256) \n",
    "\n",
    "    for files_dict in files_dicts:\n",
    "        for pid, paths in files_dict.items():\n",
    "            # Read and preprocess NIfTI file\n",
    "            img_stack = np.array(nib.load(paths[0]).get_fdata().astype(np.float32))\n",
    "\n",
    "            num_slices = img_stack.shape[2]\n",
    "            processed_img_stack = []\n",
    "            seg_stack = []\n",
    "\n",
    "            for idx in range(img_stack.shape[2]):\n",
    "                img = img_stack[:, :, idx]  # Get the current slice\n",
    "\n",
    "                im_resized = resize_image(img, target_shape)\n",
    "                \n",
    "                # Adjust contrast\n",
    "                # im_resized = cv2.convertScaleAbs(im_resized, alpha=1.465, beta=0.0) # Used alpha-value provided by Giulia (this adjusts contrast)\n",
    "\n",
    "                normed_im = im_resized / (np.max(im_resized)) \n",
    "                input_tensor = torch.from_numpy(normed_im).float()[None, None, :, :]\n",
    "\n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    pred = net(input_tensor)\n",
    "                    pred = torch.softmax(pred[0], dim=0)\n",
    "                    seg = torch.argmax(pred, dim=0).numpy()\n",
    "\n",
    "                if save_as_stack:\n",
    "                    processed_img_stack.append(im_resized)\n",
    "                    seg_stack.append(seg)\n",
    "                else:\n",
    "                    # Save\n",
    "                    pid_folder = os.path.join(output_root, str(pid))\n",
    "                    os.makedirs(pid_folder, exist_ok=True)\n",
    "\n",
    "                    affine = np.eye(4)  # identity affine \n",
    "                    #TODO: Look into this. This is an identity affine to map from numpy array to nifti file format, but we should probably\n",
    "                    # use the one from the DICOM - or does this not matter for input into Giulia's code?\n",
    "\n",
    "                    nib.save(nib.Nifti1Image(im_resized, affine), os.path.join(pid_folder, f\"{idx}_img.nii.gz\"))\n",
    "                    nib.save(nib.Nifti1Image(seg.astype(np.uint8), affine), os.path.join(pid_folder, f\"{idx}_seg.nii.gz\"))\n",
    "\n",
    "                    print(f\"Saved {pid} slice {idx}\")\n",
    "        \n",
    "            if save_as_stack and num_slices > 0: # NOTE: The order of the slices for patients with no folder structure is not necessarily correct.\n",
    "                \n",
    "                # Save the entire stack as a single NIfTI file\n",
    "                pid_folder = os.path.join(output_root, str(pid))\n",
    "                os.makedirs(pid_folder, exist_ok=True)\n",
    "\n",
    "                processed_img_stack = np.stack(processed_img_stack, axis=-1)\n",
    "                seg_stack = np.stack(seg_stack, axis=-1)\n",
    "\n",
    "                affine = np.eye(4)  # identity affine for the stack\n",
    "\n",
    "                nib.save(nib.Nifti1Image(processed_img_stack, affine), os.path.join(pid_folder, \"img_stack.nii.gz\"))\n",
    "                nib.save(nib.Nifti1Image(seg_stack.astype(np.uint8), affine), os.path.join(pid_folder, \"seg_stack.nii.gz\"))\n",
    "\n",
    "                print(f\"ID {pid}: Saved image and segmentation stacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b97738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this based on where you store the data\n",
    "base_path = os.path.abspath(\"/Users/inad001/Documents/SSCP25/Data and scripts SSCP25/ED_segmentation_data/segmentation_stacks\")\n",
    "\n",
    "cmr_files = get_cmr_nifti_files(base_path)\n",
    "\n",
    "# Run segmentation on the CMR files\n",
    "run_segmentation_nifti([cmr_files], output_root=\"/Users/inad001/Documents/SSCP25/segmented_nifti\", save_as_stack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0dd7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
