{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244138ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import pydicom\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import zoom\n",
    "from monai.bundle import load_bundle_config\n",
    "from huggingface_hub import hf_hub_download\n",
    "import cv2 #The import-call for cv2 is \"pip install opencv-python\" (not cv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da478a5",
   "metadata": {},
   "source": [
    "### Segmentation of DICOM files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ee1b5",
   "metadata": {},
   "source": [
    "Must know:\n",
    "- ID 207: Add a folder named \"sax\" in cine and move all subfolders (with the weird names) in there for consistency\n",
    "\n",
    "Good to know:\n",
    "- All missing IDs have a \"-\" in column \"Folders (y/n)\". So when you find out folder-order and first and last for the missing ones, change that to \"y\" and run the code again. \n",
    "- We should ask about 187 (no sax folder), for now it also has a \"-\" and is ignored.\n",
    "- Otherwise scroll down for the function calls. I structured it this way so it's easy to make into a .py file, but notebook is easier for debugging etc.\n",
    "- Check out the TODO tag below about the affine to convert it into Nifti format and wether that is necessary for input into Giulia's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deb5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "    Extracts sliceloc and triggertime values from a filename.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): Filename containing 'sliceloc_{val}_triggertime_{val}'.\n",
    "\n",
    "    Returns:\n",
    "        tuple[float | None, float | None]: Parsed sliceloc and triggertime as floats, or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"sliceloc_([-\\d.]+)_triggertime_([-\\d.]+)\", filename)\n",
    "    if match:\n",
    "        return float(match.group(1)), float(match.group(2))\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773df45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_files_n(df, base_path):\n",
    "    \"\"\"\n",
    "    Selects one relevant file per slice location for each patient based on ED frame and apex–base range.\n",
    "\n",
    "    For each patient, searches {base_path}/{ID}/cine/sax/ (recursively) for files named \n",
    "    like '...sliceloc_{val}_triggertime_{val}'. Keeps only slices within the apex–base \n",
    "    range and selects the earliest (ED Slice == 0) or latest frame per slice.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): df_y (see above); DataFrame of \"ED_slices_and_timepoints.csv\", without series-substructure\n",
    "        base_path (str): Root path containing the patient folders.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: Mapping from patient ID to selected file paths.\n",
    "    \"\"\"\n",
    "    patient_files = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        pid = str(row[\"ID\"]).strip()\n",
    "        try:\n",
    "            ed_slice = int(row[\"ED frame\"])\n",
    "            apex = float(row[\"apex\"])\n",
    "            base = float(row[\"base\"])\n",
    "        except (ValueError, TypeError):\n",
    "            # Skip malformed rows\n",
    "            continue\n",
    "\n",
    "        folder = os.path.join(base_path, pid, \"cine\", \"sax\")\n",
    "        if not os.path.isdir(folder):\n",
    "            print(f\"Warning: folder not found for patient {pid}\")\n",
    "            continue\n",
    "\n",
    "        files = [p for p in Path(folder).rglob(\"*\") if p.is_file()]\n",
    "        parsed = []\n",
    "\n",
    "        # Parse filenames\n",
    "        for f in files:\n",
    "            fname = f.name\n",
    "            sliceloc, triggertime = parse_filename(fname)\n",
    "            if sliceloc is not None and triggertime is not None:\n",
    "                parsed.append((f, sliceloc, triggertime))\n",
    "\n",
    "        if not parsed:\n",
    "            print(f\"Warning: no valid files for patient {pid}\")\n",
    "            continue\n",
    "\n",
    "        # Group by sliceloc → list of triggertimes\n",
    "        sliceloc_map = defaultdict(list)\n",
    "        for f, sliceloc, triggertime in parsed:\n",
    "            sliceloc_map[sliceloc].append((f, triggertime))\n",
    "\n",
    "        lower, upper = sorted([apex, base])\n",
    "        selected = []\n",
    "\n",
    "        for sliceloc, items in sliceloc_map.items():\n",
    "            if lower <= sliceloc <= upper:\n",
    "                times = [tt for _, tt in items]\n",
    "                if ed_slice == 0:\n",
    "                    target_tt = min(times)\n",
    "                else:\n",
    "                    target_tt = max(times)\n",
    "\n",
    "                # Add the file with this sliceloc + target triggertime\n",
    "                for f, tt in items:\n",
    "                    if tt == target_tt:\n",
    "                        selected.append(str(f))\n",
    "                        break  # Only one per sliceloc\n",
    "\n",
    "        patient_files[pid] = selected\n",
    "\n",
    "    return patient_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6942e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_files_y(df, base_path):\n",
    "    \"\"\"\n",
    "    Selects one relevant file per folder-defined slice for each patient using ED frame.\n",
    "\n",
    "    For each patient, looks inside {base_path}/{ID}/cine/sax/series_{folder}/ for files.\n",
    "    The slice locations are inferred from subfolder names (e.g. 'series_25').\n",
    "    The column 'folder order' lists all available series (in order),\n",
    "    while 'apex' and 'base' define the first and last folder to include.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with columns 'ID', 'ED Slice', 'apex', 'base', and 'folder order'.\n",
    "        base_path (str): Root path containing the patient folders.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: Mapping from patient ID to selected file paths.\n",
    "    \"\"\"\n",
    "\n",
    "    patient_files = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        pid = str(row[\"ID\"]).strip()\n",
    "        try:\n",
    "            ed_slice = int(row[\"ED frame\"])\n",
    "            apex = int(row[\"apex\"])\n",
    "            base = int(row[\"base\"])\n",
    "            folder_order = str(row[\"folder order\"]).strip()\n",
    "        except (ValueError, TypeError):\n",
    "            print(f\"Could not extract values for ID {pid}\")\n",
    "            continue\n",
    "\n",
    "        if not folder_order or folder_order.lower() == \"nan\":\n",
    "            continue\n",
    "\n",
    "        sax_root = os.path.join(base_path, pid, \"cine\", \"sax\")\n",
    "        if not os.path.isdir(sax_root):\n",
    "            print(f\"Warning: folder not found for patient {pid}\")\n",
    "            continue\n",
    "\n",
    "        # Get ordered folder list (as ints)\n",
    "        order = [int(x) for x in folder_order.split(\"-\") if x.isdigit()]\n",
    "        lower_idx, upper_idx = order.index(apex), order.index(base)\n",
    "        selected_series = order[lower_idx:upper_idx+1]\n",
    "\n",
    "        selected = []\n",
    "\n",
    "        for sliceloc in selected_series:\n",
    "            series_path = os.path.join(sax_root, f\"series_{sliceloc}\")\n",
    "            if not os.path.isdir(series_path):\n",
    "                print(f\"Warning: missing folder series_{sliceloc} for patient {pid}\")\n",
    "                continue\n",
    "\n",
    "            files = glob.glob(os.path.join(series_path, \"*\"))\n",
    "            triggertimes = []\n",
    "\n",
    "            for f in files:\n",
    "                _, tt = parse_filename(os.path.basename(f))\n",
    "                if tt is not None:\n",
    "                    triggertimes.append((f, tt))\n",
    "\n",
    "            if not triggertimes:\n",
    "                continue\n",
    "\n",
    "            if ed_slice == 0:\n",
    "                chosen_file = min(triggertimes, key=lambda x: x[1])[0]\n",
    "            else:\n",
    "                chosen_file = max(triggertimes, key=lambda x: x[1])[0]\n",
    "\n",
    "            selected.append(chosen_file)\n",
    "\n",
    "        patient_files[pid] = selected\n",
    "\n",
    "    return patient_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(img, target_shape):\n",
    "    \"\"\"\n",
    "    Resize a 2D image to the target shape using OpenCV.\n",
    "\n",
    "    Parameters:\n",
    "        img (np.ndarray): Input image to resize.\n",
    "        target_shape (tuple[int, int]): Desired output shape (height, width).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Resized image.\n",
    "    \"\"\"\n",
    "    # Add padding if one of the image dimensions are smaller than the target shape\n",
    "    if img.shape[0] < target_shape[0] or img.shape[1] < target_shape[1]:\n",
    "\n",
    "        h, w = img.shape\n",
    "\n",
    "        if h < target_shape[0]:\n",
    "            pad_top = (target_shape[0] - h) // 2\n",
    "            pad_bottom = target_shape[0] - h - pad_top\n",
    "        else:\n",
    "            pad_top = 0\n",
    "            pad_bottom = 0\n",
    "\n",
    "\n",
    "        if w < target_shape[1]:\n",
    "            pad_left = (target_shape[1] - w) // 2\n",
    "            pad_right = target_shape[1] - w - pad_left\n",
    "        else:\n",
    "            pad_left = 0\n",
    "            pad_right = 0\n",
    "\n",
    "\n",
    "        img = cv2.copyMakeBorder(\n",
    "                        img,\n",
    "                        top=pad_top,\n",
    "                        bottom=pad_bottom,\n",
    "                        left=pad_left,\n",
    "                        right=pad_right,\n",
    "                        borderType=cv2.BORDER_CONSTANT,\n",
    "                        value=0  # or use the image mean\n",
    "                    )\n",
    "    \n",
    "    # Resize the image if it exceeds the target shape\n",
    "    if img.shape[0] > target_shape[0] or img.shape[1] > target_shape[1]:\n",
    "        img = cv2.resize(img, (target_shape[1], target_shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_segmentation(files_dicts, output_root, save_as_stack: bool):\n",
    "    \"\"\"\n",
    "    Runs MONAI ventricular segmentation on all DICOM files provided in files_dicts.\n",
    "\n",
    "    Parameters:\n",
    "        files_dicts (list[dict]): List of dicts (e.g., [files_n, files_y]) with {pid: [file_paths]}.\n",
    "        output_root (str): Root folder where output NIfTI files will be saved.\n",
    "        save_as_stack (bool): If True, saves all slices of a patient as a single NIfTI stack.\n",
    "    \"\"\"\n",
    "    # Load MONAI network config & weights\n",
    "    parser = load_bundle_config(\"MONAI\", \"train.json\")\n",
    "    net = parser.get_parsed_content(\"network_def\")\n",
    "\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=\"MONAI/ventricular_short_axis_3label\",\n",
    "        filename=\"models/model.pt\"\n",
    "    )\n",
    "    net.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    net.eval()\n",
    "\n",
    "    target_shape = (256, 256) \n",
    "\n",
    "    for files_dict in files_dicts:\n",
    "        for pid, paths in files_dict.items():\n",
    "            num_slices = len(paths)\n",
    "            img_stack = []\n",
    "            seg_stack = []\n",
    "            for idx, path in enumerate(paths):\n",
    "                # Read and preprocess DICOM\n",
    "                ds = pydicom.dcmread(path)\n",
    "                img = ds.pixel_array.astype(np.float32)\n",
    "\n",
    "                im_resized = resize_image(img, target_shape)\n",
    "\n",
    "                # Normalize and add batch & channel dims\n",
    "                normed_im = im_resized / im_resized.max()\n",
    "                input_tensor = torch.from_numpy(normed_im).float()[None, None, :, :]\n",
    "\n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    pred = net(input_tensor)\n",
    "                    pred = torch.softmax(pred[0], dim=0)\n",
    "                    seg = torch.argmax(pred, dim=0).numpy()\n",
    "\n",
    "                if save_as_stack:\n",
    "                    img_stack.append(im_resized)\n",
    "                    seg_stack.append(seg)\n",
    "                else:\n",
    "                    # Save\n",
    "                    pid_folder = os.path.join(output_root, str(pid))\n",
    "                    os.makedirs(pid_folder, exist_ok=True)\n",
    "\n",
    "                    affine = np.eye(4)  # identity affine \n",
    "                    #TODO: Look into this. This is an identity affine to map from numpy array to nifti file format, but we should probably\n",
    "                    # use the one from the DICOM - or does this not matter for input into Giulia's code?\n",
    "\n",
    "                    nib.save(nib.Nifti1Image(im_resized, affine), os.path.join(pid_folder, f\"{idx}_img.nii.gz\"))\n",
    "                    nib.save(nib.Nifti1Image(seg.astype(np.uint8), affine), os.path.join(pid_folder, f\"{idx}_seg.nii.gz\"))\n",
    "\n",
    "                    print(f\"Saved {pid} slice {idx}\")\n",
    "        \n",
    "            if save_as_stack and num_slices > 0: # NOTE: The order of the slices for patients with no folder structure is not necessarily correct.\n",
    "                \n",
    "                # Save the entire stack as a single NIfTI file\n",
    "                pid_folder = os.path.join(output_root, str(pid))\n",
    "                os.makedirs(pid_folder, exist_ok=True)\n",
    "\n",
    "                img_stack = np.stack(img_stack, axis=-1)\n",
    "                seg_stack = np.stack(seg_stack, axis=-1)\n",
    "\n",
    "                affine = np.eye(4)  # identity affine for the stack\n",
    "\n",
    "                nib.save(nib.Nifti1Image(img_stack, affine), os.path.join(pid_folder, \"img_stack.nii.gz\"))\n",
    "                nib.save(nib.Nifti1Image(seg_stack.astype(np.uint8), affine), os.path.join(pid_folder, \"seg_stack.nii.gz\"))\n",
    "\n",
    "                print(f\"ID {pid}: Saved image and segmentation stacks\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73109e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be main in .py\n",
    "\n",
    "# read in csv split on folders y/n\n",
    "csv_file = \"ED_slices_and_timepoints.csv\" #For the future, once we structure our folders/files better we need to (probably) adjust this import\n",
    "df = pd.read_csv(csv_file)\n",
    "#display(df)\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "df[\"Folders (y/n)\"] = df[\"Folders (y/n)\"].str.strip().str.lower()\n",
    "\n",
    "df_y = df[df[\"Folders (y/n)\"] == 'y'].reset_index(drop=True)\n",
    "df_n = df[df[\"Folders (y/n)\"] == 'n'].reset_index(drop=True)\n",
    "\n",
    "#display(df_n)\n",
    "#display(df_y)\n",
    "\n",
    "# Change this based on where you store the data\n",
    "base_path = \"/Users/inad001/Documents/SSCP25/Data and scripts SSCP25/CMR_image_data/new data-dicom\"\n",
    "\n",
    "files_n = get_relevant_files_n(df_n, base_path)\n",
    "files_y = get_relevant_files_y(df_y, base_path)\n",
    "\n",
    "# Call segmentation and save segmentations and images under specified output file (change to match your own destination)\n",
    "run_segmentation([files_n, files_y], output_root=\"/Users/inad001/Documents/SSCP25/segmented_data\", save_as_stack=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e549c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_n.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e87a9f1",
   "metadata": {},
   "source": [
    "### Create h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from myo_cavity import create_h5_file\n",
    "\n",
    "patients = list(files_n.keys()) + list(files_y.keys())\n",
    "segmentation_base_path = \"/Users/inad001/Documents/SSCP25/segmented_data\"\n",
    "ouput_path = \"seg_files/\"\n",
    "\n",
    "create_h5_file(patients=patients, base_path=segmentation_base_path, output_dir=ouput_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41a0e40",
   "metadata": {},
   "source": [
    "### Segmentation of NIfTI files\n",
    "\n",
    "This part collects CMR NIfTI files for the patients located in the ED_segmentation_data folder and predicts a segmentation for each patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmr_nifti_files(base_path):\n",
    "    \"\"\"\n",
    "    Gets all CMR NIfTI files for all patients in the specified base path.\n",
    "\n",
    "    Parameters:\n",
    "        base_path (str): Root path containing the patient folders.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: Mapping from patient ID to lists of file paths.\n",
    "    \"\"\"\n",
    "    patient_files = {}\n",
    "\n",
    "    for patient in os.listdir(base_path):\n",
    "        pid = patient.strip()\n",
    "\n",
    "        patient_path = os.path.join(base_path, pid)\n",
    "\n",
    "        if not os.path.isdir(patient_path):\n",
    "            print(f\"Warning: {patient_path} is not a directory\")\n",
    "            continue\n",
    "\n",
    "        files = [os.path.join(patient_path, f) for f in os.listdir(patient_path) if f.startswith(\"cmr\") and f.endswith(\".nii\")]\n",
    "\n",
    "        if not files:\n",
    "            print(f\"Warning: no valid files found for patient {pid}\")\n",
    "            continue\n",
    "\n",
    "        patient_files[pid] = files\n",
    "    \n",
    "    return patient_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8051de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_segmentation_nifti(files_dicts, output_root, save_as_stack: bool):\n",
    "    \"\"\"\n",
    "    Runs MONAI ventricular segmentation on all CMR NIfTI files provided in files_dicts.\n",
    "\n",
    "    Parameters:\n",
    "        files_dicts (list[dict]): List of dicts (e.g., [files_n, files_y]) with {pid: [file_paths]}.\n",
    "        output_root (str): Root folder where output NIfTI files will be saved.\n",
    "        save_as_stack (bool): If True, saves all slices of a patient as a single NIfTI stack.\n",
    "    \"\"\"\n",
    "    # Load MONAI network config & weights\n",
    "    parser = load_bundle_config(\"MONAI\", \"train.json\")\n",
    "    net = parser.get_parsed_content(\"network_def\")\n",
    "\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=\"MONAI/ventricular_short_axis_3label\",\n",
    "        filename=\"models/model.pt\"\n",
    "    )\n",
    "    net.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    net.eval()\n",
    "\n",
    "    target_shape = (256, 256) \n",
    "\n",
    "    for files_dict in files_dicts:\n",
    "        for pid, paths in files_dict.items():\n",
    "            # Read and preprocess NIfTI file\n",
    "            img_stack = np.array(nib.load(paths[0]).get_fdata().astype(np.float32))\n",
    "\n",
    "            num_slices = img_stack.shape[2]\n",
    "            processed_img_stack = []\n",
    "            seg_stack = []\n",
    "\n",
    "            for idx in range(img_stack.shape[2]):\n",
    "                img = img_stack[:, :, idx]  # Get the current slice\n",
    "\n",
    "                im_resized = resize_image(img, target_shape)\n",
    "                \n",
    "                # Adjust contrast\n",
    "                # im_resized = cv2.convertScaleAbs(im_resized, alpha=1.465, beta=0.0) # Used alpha-value provided by Giulia (this adjusts contrast)\n",
    "\n",
    "                normed_im = im_resized / (np.max(im_resized)) \n",
    "                input_tensor = torch.from_numpy(normed_im).float()[None, None, :, :]\n",
    "\n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    pred = net(input_tensor)\n",
    "                    pred = torch.softmax(pred[0], dim=0)\n",
    "                    seg = torch.argmax(pred, dim=0).numpy()\n",
    "\n",
    "                if save_as_stack:\n",
    "                    processed_img_stack.append(im_resized)\n",
    "                    seg_stack.append(seg)\n",
    "                else:\n",
    "                    # Save\n",
    "                    pid_folder = os.path.join(output_root, str(pid))\n",
    "                    os.makedirs(pid_folder, exist_ok=True)\n",
    "\n",
    "                    affine = np.eye(4)  # identity affine \n",
    "                    #TODO: Look into this. This is an identity affine to map from numpy array to nifti file format, but we should probably\n",
    "                    # use the one from the DICOM - or does this not matter for input into Giulia's code?\n",
    "\n",
    "                    nib.save(nib.Nifti1Image(im_resized, affine), os.path.join(pid_folder, f\"{idx}_img.nii.gz\"))\n",
    "                    nib.save(nib.Nifti1Image(seg.astype(np.uint8), affine), os.path.join(pid_folder, f\"{idx}_seg.nii.gz\"))\n",
    "\n",
    "                    print(f\"Saved {pid} slice {idx}\")\n",
    "        \n",
    "            if save_as_stack and num_slices > 0: # NOTE: The order of the slices for patients with no folder structure is not necessarily correct.\n",
    "                \n",
    "                # Save the entire stack as a single NIfTI file\n",
    "                pid_folder = os.path.join(output_root, str(pid))\n",
    "                os.makedirs(pid_folder, exist_ok=True)\n",
    "\n",
    "                processed_img_stack = np.stack(processed_img_stack, axis=-1)\n",
    "                seg_stack = np.stack(seg_stack, axis=-1)\n",
    "\n",
    "                affine = np.eye(4)  # identity affine for the stack\n",
    "\n",
    "                nib.save(nib.Nifti1Image(processed_img_stack, affine), os.path.join(pid_folder, \"img_stack.nii.gz\"))\n",
    "                nib.save(nib.Nifti1Image(seg_stack.astype(np.uint8), affine), os.path.join(pid_folder, \"seg_stack.nii.gz\"))\n",
    "\n",
    "                print(f\"ID {pid}: Saved image and segmentation stacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b97738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this based on where you store the data\n",
    "base_path = os.path.abspath(\"/Users/inad001/Documents/SSCP25/Data and scripts SSCP25/ED_segmentation_data/segmentation_stacks\")\n",
    "\n",
    "cmr_files = get_cmr_nifti_files(base_path)\n",
    "\n",
    "# Run segmentation on the CMR files\n",
    "run_segmentation_nifti([cmr_files], output_root=\"/Users/inad001/Documents/SSCP25/segmented_nifti\", save_as_stack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0dd7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sscp25-vTSm5jc8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
