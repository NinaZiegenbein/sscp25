{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244138ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import pydicom\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import zoom\n",
    "from monai.bundle import load_bundle_config\n",
    "from huggingface_hub import hf_hub_download\n",
    "import cv2 #The import-call for cv2 is \"pip install opencv-python\" (not cv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ee1b5",
   "metadata": {},
   "source": [
    "Must know:\n",
    "- ID 207: Add a folder named \"sax\" in cine and move all subfolders (with the weird names) in there for consistency\n",
    "\n",
    "Good to know:\n",
    "- All missing IDs have a \"-\" in column \"Folders (y/n)\". So when you find out folder-order and first and last for the missing ones, change that to \"y\" and run the code again. \n",
    "- We should ask about 187 (no sax folder), for now it also has a \"-\" and is ignored.\n",
    "- Otherwise scroll down for the function calls. I structured it this way so it's easy to make into a .py file, but notebook is easier for debugging etc.\n",
    "- Check out the TODO tag below about the affine to convert it into Nifti format and wether that is necessary for input into Giulia's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deb5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "    Extracts sliceloc and triggertime values from a filename.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): Filename containing 'sliceloc_{val}_triggertime_{val}'.\n",
    "\n",
    "    Returns:\n",
    "        tuple[float | None, float | None]: Parsed sliceloc and triggertime as floats, or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"sliceloc_([-\\d.]+)_triggertime_([-\\d.]+)\", filename)\n",
    "    if match:\n",
    "        return float(match.group(1)), float(match.group(2))\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773df45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_files_n(df, base_path):\n",
    "    \"\"\"\n",
    "    Selects one relevant file per slice location for each patient based on ED frame and apex–base range.\n",
    "\n",
    "    For each patient, searches {base_path}/{ID}/cine/sax/ (recursively) for files named \n",
    "    like '...sliceloc_{val}_triggertime_{val}'. Keeps only slices within the apex–base \n",
    "    range and selects the earliest (ED Slice == 0) or latest frame per slice.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): df_y (see above); DataFrame of \"ED_slices_and_timepoints.csv\", without series-substructure\n",
    "        base_path (str): Root path containing the patient folders.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: Mapping from patient ID to selected file paths.\n",
    "    \"\"\"\n",
    "    patient_files = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        pid = str(row[\"ID\"]).strip()\n",
    "        try:\n",
    "            ed_slice = int(row[\"ED frame\"])\n",
    "            apex = float(row[\"apex\"])\n",
    "            base = float(row[\"base\"])\n",
    "        except (ValueError, TypeError):\n",
    "            # Skip malformed rows\n",
    "            continue\n",
    "\n",
    "        folder = os.path.join(base_path, pid, \"cine\", \"sax\")\n",
    "        if not os.path.isdir(folder):\n",
    "            print(f\"Warning: folder not found for patient {pid}\")\n",
    "            continue\n",
    "\n",
    "        files = [p for p in Path(folder).rglob(\"*\") if p.is_file()]\n",
    "        parsed = []\n",
    "\n",
    "        # Parse filenames\n",
    "        for f in files:\n",
    "            fname = f.name\n",
    "            sliceloc, triggertime = parse_filename(fname)\n",
    "            if sliceloc is not None and triggertime is not None:\n",
    "                parsed.append((f, sliceloc, triggertime))\n",
    "\n",
    "        if not parsed:\n",
    "            print(f\"Warning: no valid files for patient {pid}\")\n",
    "            continue\n",
    "\n",
    "        # Group by sliceloc → list of triggertimes\n",
    "        sliceloc_map = defaultdict(list)\n",
    "        for f, sliceloc, triggertime in parsed:\n",
    "            sliceloc_map[sliceloc].append((f, triggertime))\n",
    "\n",
    "        lower, upper = sorted([apex, base])\n",
    "        selected = []\n",
    "\n",
    "        for sliceloc, items in sliceloc_map.items():\n",
    "            if lower <= sliceloc <= upper:\n",
    "                times = [tt for _, tt in items]\n",
    "                if ed_slice == 0:\n",
    "                    target_tt = min(times)\n",
    "                else:\n",
    "                    target_tt = max(times)\n",
    "\n",
    "                # Add the file with this sliceloc + target triggertime\n",
    "                for f, tt in items:\n",
    "                    if tt == target_tt:\n",
    "                        selected.append(str(f))\n",
    "                        break  # Only one per sliceloc\n",
    "\n",
    "        patient_files[pid] = selected\n",
    "\n",
    "    return patient_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6942e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_files_y(df, base_path):\n",
    "    \"\"\"\n",
    "    Selects one relevant file per folder-defined slice for each patient using ED frame.\n",
    "\n",
    "    For each patient, looks inside {base_path}/{ID}/cine/sax/series_{folder}/ for files.\n",
    "    The slice locations are inferred from subfolder names (e.g. 'series_25').\n",
    "    The column 'folder order' lists all available series (in order),\n",
    "    while 'apex' and 'base' define the first and last folder to include.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with columns 'ID', 'ED Slice', 'apex', 'base', and 'folder order'.\n",
    "        base_path (str): Root path containing the patient folders.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: Mapping from patient ID to selected file paths.\n",
    "    \"\"\"\n",
    "\n",
    "    patient_files = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        pid = str(row[\"ID\"]).strip()\n",
    "        try:\n",
    "            ed_slice = int(row[\"ED frame\"])\n",
    "            apex = int(row[\"apex\"])\n",
    "            base = int(row[\"base\"])\n",
    "            folder_order = str(row[\"folder order\"]).strip()\n",
    "        except (ValueError, TypeError):\n",
    "            print(f\"Could not extract values for ID {pid}\")\n",
    "            continue\n",
    "\n",
    "        if not folder_order or folder_order.lower() == \"nan\":\n",
    "            continue\n",
    "\n",
    "        sax_root = os.path.join(base_path, pid, \"cine\", \"sax\")\n",
    "        if not os.path.isdir(sax_root):\n",
    "            print(f\"Warning: folder not found for patient {pid}\")\n",
    "            continue\n",
    "\n",
    "        # Get ordered folder list (as ints)\n",
    "        order = [int(x) for x in folder_order.split(\"-\") if x.isdigit()]\n",
    "        lower, upper = sorted([apex, base])\n",
    "        lower_idx, upper_idx = order.index(lower), order.index(upper)\n",
    "        selected_series = order[lower_idx:upper_idx+1]\n",
    "\n",
    "        selected = []\n",
    "\n",
    "        for sliceloc in selected_series:\n",
    "            series_path = os.path.join(sax_root, f\"series_{sliceloc}\")\n",
    "            if not os.path.isdir(series_path):\n",
    "                print(f\"Warning: missing folder series_{sliceloc} for patient {pid}\")\n",
    "                continue\n",
    "\n",
    "            files = glob.glob(os.path.join(series_path, \"*\"))\n",
    "            triggertimes = []\n",
    "\n",
    "            for f in files:\n",
    "                _, tt = parse_filename(os.path.basename(f))\n",
    "                if tt is not None:\n",
    "                    triggertimes.append((f, tt))\n",
    "\n",
    "            if not triggertimes:\n",
    "                continue\n",
    "\n",
    "            if ed_slice == 0:\n",
    "                chosen_file = min(triggertimes, key=lambda x: x[1])[0]\n",
    "            else:\n",
    "                chosen_file = max(triggertimes, key=lambda x: x[1])[0]\n",
    "\n",
    "            selected.append(chosen_file)\n",
    "\n",
    "        patient_files[pid] = selected\n",
    "\n",
    "    return patient_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_segmentation(files_dicts, output_root, save_as_stack: bool):\n",
    "    \"\"\"\n",
    "    Runs MONAI ventricular segmentation on all DICOM files provided in files_dicts.\n",
    "\n",
    "    Parameters:\n",
    "        files_dicts (list[dict]): List of dicts (e.g., [files_n, files_y]) with {pid: [file_paths]}.\n",
    "        output_root (str): Root folder where output NIfTI files will be saved.\n",
    "    \"\"\"\n",
    "    # Load MONAI network config & weights\n",
    "    parser = load_bundle_config(\"MONAI\", \"train.json\")\n",
    "    net = parser.get_parsed_content(\"network_def\")\n",
    "\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=\"MONAI/ventricular_short_axis_3label\",\n",
    "        filename=\"models/model.pt\"\n",
    "    )\n",
    "    net.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    net.eval()\n",
    "\n",
    "    target_shape = (256, 256) \n",
    "\n",
    "    for files_dict in files_dicts:\n",
    "        for pid, paths in files_dict.items():\n",
    "            num_slices = len(paths)\n",
    "            img_stack = []\n",
    "            seg_stack = []\n",
    "            for idx, path in enumerate(paths):\n",
    "                # Read and preprocess DICOM\n",
    "                ds = pydicom.dcmread(path)\n",
    "                img = ds.pixel_array.astype(np.float32)\n",
    "\n",
    "                #print(f\"Original image shape: {img.shape}, ndim: {img.ndim}\")\n",
    "\n",
    "                # Resample width and height to fixed size (256, 256) \n",
    "                im_resized = cv2.resize(img, (target_shape[1], target_shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "                # Adjust contrast\n",
    "                # im_resized = cv2.convertScaleAbs(im_resized, alpha=1.465, beta=0.0) # Used alpha-value provided by Giulia (this adjusts contrast)\n",
    "\n",
    "                #print(f\"Resized image shape: {im_resized.shape}\")\n",
    "\n",
    "                # Normalize and add batch & channel dims\n",
    "                normed_im = im_resized / im_resized.max()\n",
    "                input_tensor = torch.from_numpy(normed_im).float()[None, None, :, :]\n",
    "\n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    pred = net(input_tensor)\n",
    "                    pred = torch.softmax(pred[0], dim=0)\n",
    "                    seg = torch.argmax(pred, dim=0).numpy()\n",
    "\n",
    "                if save_as_stack:\n",
    "                    img_stack.append(normed_im)\n",
    "                    seg_stack.append(seg)\n",
    "                else:\n",
    "                    # Save\n",
    "                    pid_folder = os.path.join(output_root, str(pid))\n",
    "                    os.makedirs(pid_folder, exist_ok=True)\n",
    "\n",
    "                    affine = np.eye(4)  # identity affine \n",
    "                    #TODO: Look into this. This is an identity affine to map from numpy array to nifti file format, but we should probably\n",
    "                    # use the one from the DICOM - or does this not matter for input into Giulia's code?\n",
    "\n",
    "                    nib.save(nib.Nifti1Image(normed_im, affine), os.path.join(pid_folder, f\"{idx}_img.nii.gz\"))\n",
    "                    nib.save(nib.Nifti1Image(seg.astype(np.uint8), affine), os.path.join(pid_folder, f\"{idx}_seg.nii.gz\"))\n",
    "\n",
    "                    print(f\"Saved {pid} slice {idx}\")\n",
    "        \n",
    "            if save_as_stack and num_slices > 0: # NOTE: The order of the slices for patients with no folder structure is not necessarily correct.\n",
    "                \n",
    "                # Save the entire stack as a single NIfTI file\n",
    "                pid_folder = os.path.join(output_root, str(pid))\n",
    "                os.makedirs(pid_folder, exist_ok=True)\n",
    "\n",
    "                img_stack = np.stack(img_stack, axis=-1)\n",
    "                seg_stack = np.stack(seg_stack, axis=-1)\n",
    "\n",
    "                affine = np.eye(4)  # identity affine for the stack\n",
    "\n",
    "                nib.save(nib.Nifti1Image(img_stack, affine), os.path.join(pid_folder, \"img_stack.nii.gz\"))\n",
    "                nib.save(nib.Nifti1Image(seg_stack.astype(np.uint8), affine), os.path.join(pid_folder, \"seg_stack.nii.gz\"))\n",
    "\n",
    "                print(f\"Saved {pid} image and segmentation stacks\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "73109e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 15 slice 0\n",
      "Saved 15 slice 1\n",
      "Saved 15 slice 2\n",
      "Saved 15 slice 3\n",
      "Saved 15 slice 4\n",
      "Saved 15 slice 5\n",
      "Saved 15 slice 6\n",
      "Saved 15 slice 7\n",
      "Saved 114 slice 0\n",
      "Saved 114 slice 1\n",
      "Saved 114 slice 2\n",
      "Saved 114 slice 3\n",
      "Saved 114 slice 4\n",
      "Saved 114 slice 5\n",
      "Saved 114 slice 6\n",
      "Saved 114 slice 7\n",
      "Saved 114 slice 8\n",
      "Saved 114 slice 9\n",
      "Saved 114 slice 10\n",
      "Saved 114 slice 11\n",
      "Saved 126 slice 0\n",
      "Saved 126 slice 1\n",
      "Saved 126 slice 2\n",
      "Saved 126 slice 3\n",
      "Saved 126 slice 4\n",
      "Saved 126 slice 5\n",
      "Saved 126 slice 6\n",
      "Saved 126 slice 7\n",
      "Saved 126 slice 8\n",
      "Saved 126 slice 9\n",
      "Saved 130 slice 0\n",
      "Saved 130 slice 1\n",
      "Saved 130 slice 2\n",
      "Saved 130 slice 3\n",
      "Saved 130 slice 4\n",
      "Saved 130 slice 5\n",
      "Saved 130 slice 6\n",
      "Saved 130 slice 7\n",
      "Saved 130 slice 8\n",
      "Saved 130 slice 9\n",
      "Saved 130 slice 10\n",
      "Saved 130 slice 11\n",
      "Saved 138 slice 0\n",
      "Saved 138 slice 1\n",
      "Saved 138 slice 2\n",
      "Saved 138 slice 3\n",
      "Saved 138 slice 4\n",
      "Saved 138 slice 5\n",
      "Saved 138 slice 6\n",
      "Saved 163 slice 0\n",
      "Saved 163 slice 1\n",
      "Saved 163 slice 2\n",
      "Saved 163 slice 3\n",
      "Saved 163 slice 4\n",
      "Saved 163 slice 5\n",
      "Saved 163 slice 6\n",
      "Saved 163 slice 7\n",
      "Saved 163 slice 8\n",
      "Saved 207 slice 0\n",
      "Saved 207 slice 1\n",
      "Saved 207 slice 2\n",
      "Saved 207 slice 3\n",
      "Saved 207 slice 4\n",
      "Saved 207 slice 5\n",
      "Saved 207 slice 6\n",
      "Saved 207 slice 7\n",
      "Saved 207 slice 8\n",
      "Saved 207 slice 9\n",
      "Saved 229 slice 0\n",
      "Saved 229 slice 1\n",
      "Saved 229 slice 2\n",
      "Saved 229 slice 3\n",
      "Saved 229 slice 4\n",
      "Saved 229 slice 5\n",
      "Saved 229 slice 6\n",
      "Saved 229 slice 7\n",
      "Saved 229 slice 8\n",
      "Saved 304 slice 0\n",
      "Saved 304 slice 1\n",
      "Saved 304 slice 2\n",
      "Saved 304 slice 3\n",
      "Saved 304 slice 4\n",
      "Saved 304 slice 5\n",
      "Saved 304 slice 6\n",
      "Saved 304 slice 7\n",
      "Saved 304 slice 8\n",
      "Saved 11 slice 0\n",
      "Saved 11 slice 1\n",
      "Saved 11 slice 2\n",
      "Saved 11 slice 3\n",
      "Saved 11 slice 4\n",
      "Saved 11 slice 5\n",
      "Saved 11 slice 6\n",
      "Saved 11 slice 7\n",
      "Saved 11 slice 8\n",
      "Saved 173 slice 0\n",
      "Saved 173 slice 1\n",
      "Saved 173 slice 2\n",
      "Saved 173 slice 3\n",
      "Saved 173 slice 4\n",
      "Saved 173 slice 5\n",
      "Saved 173 slice 6\n",
      "Saved 190 slice 0\n",
      "Saved 190 slice 1\n",
      "Saved 190 slice 2\n",
      "Saved 190 slice 3\n",
      "Saved 190 slice 4\n",
      "Saved 190 slice 5\n",
      "Saved 190 slice 6\n",
      "Saved 190 slice 7\n",
      "Saved 198 slice 0\n",
      "Saved 198 slice 1\n",
      "Saved 198 slice 2\n",
      "Saved 198 slice 3\n",
      "Saved 198 slice 4\n",
      "Saved 198 slice 5\n",
      "Saved 198 slice 6\n",
      "Saved 198 slice 7\n",
      "Saved 202 slice 0\n",
      "Saved 202 slice 1\n",
      "Saved 202 slice 2\n",
      "Saved 202 slice 3\n",
      "Saved 202 slice 4\n",
      "Saved 202 slice 5\n",
      "Saved 202 slice 6\n",
      "Saved 202 slice 7\n",
      "Saved 202 slice 8\n",
      "Saved 204 slice 0\n",
      "Saved 204 slice 1\n",
      "Saved 204 slice 2\n",
      "Saved 204 slice 3\n",
      "Saved 204 slice 4\n",
      "Saved 204 slice 5\n",
      "Saved 218 slice 0\n",
      "Saved 218 slice 1\n",
      "Saved 218 slice 2\n",
      "Saved 218 slice 3\n",
      "Saved 218 slice 4\n",
      "Saved 218 slice 5\n"
     ]
    }
   ],
   "source": [
    "# This would be main in .py\n",
    "\n",
    "# read in csv split on folders y/n\n",
    "csv_file = \"ED_slices_and_timepoints.csv\" #For the future, once we structure our folders/files better we need to (probably) adjust this import\n",
    "df = pd.read_csv(csv_file)\n",
    "#display(df)\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "df[\"Folders (y/n)\"] = df[\"Folders (y/n)\"].str.strip().str.lower()\n",
    "\n",
    "df_y = df[df[\"Folders (y/n)\"] == 'y'].reset_index(drop=True)\n",
    "df_n = df[df[\"Folders (y/n)\"] == 'n'].reset_index(drop=True)\n",
    "\n",
    "#display(df_n)\n",
    "#display(df_y)\n",
    "\n",
    "# Change this based on where you store the data\n",
    "# base_path = \"/Users/au698484/Documents/SSCP25_data/Data and scripts SSCP25 3/CMR_image_data/new data-dicom\"\n",
    "base_path = '/Users/inad001/Documents/SSCP25/Data and scripts SSCP25/CMR_image_data/new data-dicom'\n",
    "\n",
    "files_n = get_relevant_files_n(df_n, base_path)\n",
    "files_y = get_relevant_files_y(df_y, base_path)\n",
    "\n",
    "# Call segmentation and save segmentations and images under specified output file (change to match your own destination)\n",
    "# run_segmentation([files_n, files_y], output_root=\"/Users/au698484/Documents/SSCP25_data_segmented\")\n",
    "run_segmentation([files_n, files_y], output_root=\"/Users/inad001/Documents/SSCP25/segmented_data\", save_as_stack=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41a0e40",
   "metadata": {},
   "source": [
    "### ED_segmentation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7e3cab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.abspath(\"/Users/inad001/Documents/SSCP25/Data and scripts SSCP25/ED_segmentation_data/segmentation_stacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "92ac66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmr_nifti_files(base_path):\n",
    "\n",
    "    patient_files = {}\n",
    "\n",
    "    for patient in os.listdir(base_path):\n",
    "        pid = patient.strip()\n",
    "\n",
    "        patient_path = os.path.join(base_path, pid)\n",
    "\n",
    "        if not os.path.isdir(patient_path):\n",
    "            print(f\"Warning: {patient_path} is not a directory\")\n",
    "            continue\n",
    "\n",
    "        files = [os.path.join(patient_path, f) for f in os.listdir(patient_path) if f.startswith(\"cmr\") and f.endswith(\".nii\")]\n",
    "\n",
    "        if not files:\n",
    "            print(f\"Warning: no valid files found for patient {pid}\")\n",
    "            continue\n",
    "\n",
    "        patient_files[pid] = files\n",
    "    \n",
    "    return patient_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8051de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_segmentation_nifti(files_dicts, output_root):\n",
    "    \"\"\"\n",
    "    Runs MONAI ventricular segmentation on all DICOM files provided in files_dicts.\n",
    "\n",
    "    Parameters:\n",
    "        files_dicts (list[dict]): List of dicts (e.g., [files_n, files_y]) with {pid: [file_paths]}.\n",
    "        output_root (str): Root folder where output NIfTI files will be saved.\n",
    "    \"\"\"\n",
    "    # Load MONAI network config & weights\n",
    "    parser = load_bundle_config(\"MONAI\", \"train.json\")\n",
    "    net = parser.get_parsed_content(\"network_def\")\n",
    "\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=\"MONAI/ventricular_short_axis_3label\",\n",
    "        filename=\"models/model.pt\"\n",
    "    )\n",
    "    net.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    net.eval()\n",
    "\n",
    "    target_shape = (256, 256) \n",
    "\n",
    "    for files_dict in files_dicts:\n",
    "        for pid, paths in files_dict.items():\n",
    "\n",
    "            num_files = len(paths)\n",
    "\n",
    "            if num_files == 0:\n",
    "                print(f\"No files found for patient {pid}. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            imgs = []\n",
    "            if paths[0].endswith('.nii'): # TODO: Don't do this if the list is empty\n",
    "                # Read and preprocess NIfTI file\n",
    "                # nifti = nib.load(paths[0])\n",
    "                img_stack = np.array(nib.load(paths[0]).get_fdata().astype(np.float32))\n",
    "                # imgs.append(img_stack.unsqueeze(-1))\n",
    "            else:\n",
    "                # Create an empty stack for DICOM images\n",
    "                img_stack = np.zeros((target_shape[0], target_shape[1], num_files), dtype=np.float32)\n",
    "                for idx, path in enumerate(paths):\n",
    "                    # Read and preprocess DICOM\n",
    "                    ds = pydicom.dcmread(path)\n",
    "                    img = ds.pixel_array.astype(np.float32)\n",
    "                    im_resized = cv2.resize(img, (target_shape[1], target_shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "                    img_stack[:, :, idx] = im_resized\n",
    "\n",
    "            seg_stack = np.zeros((target_shape[1], target_shape[1], img_stack.shape[2]), dtype=np.int64)\n",
    "            processed_imgs_stack = np.zeros((target_shape[1], target_shape[1], img_stack.shape[2]), dtype=np.float32)  # Use the same shape as img_stack\n",
    "            # seg_stack = np.zeros_like(img_stack, dtype=np.int64)  # Use the same shape as img_stack\n",
    "            segs = []\n",
    "            for idx in range(img_stack.shape[2]):\n",
    "                img = img_stack[:, :, idx]  # Get the current slice\n",
    "                #print(f\"Original image shape: {img.shape}, ndim: {img.ndim}\")\n",
    "\n",
    "                if img.shape[0] != target_shape[0] or img.shape[1] != target_shape[1]:\n",
    "                    img = cv2.resize(img, (target_shape[1], target_shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "\n",
    "                # Resample width and height to fixed size (256, 256) \n",
    "                # im_resized = cv2.resize(img, (target_shape[1], target_shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "                # Adjust contrast\n",
    "                # img = cv2.convertScaleAbs(img, alpha=1.465, beta=0.0) # Used alpha-value provided by Giulia (this adjusts contrast)\n",
    "                # img = cv2.convertScaleAbs(img, alpha=1.1, beta=0.0) # Used alpha-value provided by Giulia (this adjusts contrast), alpha \n",
    "\n",
    "                #print(f\"Resized image shape: {im_resized.shape}\")\n",
    "\n",
    "                # Normalize and add batch & channel dims\n",
    "                # input_tensor = torch.from_numpy(im_resized / im_resized.max()).float()[None, None, :, :]\n",
    "                input_tensor = torch.from_numpy(img / img.max()).float()[None, None, :, :]\n",
    "                # input_tensor = torch.from_numpy(img).float()[None, None, :, :]\n",
    "\n",
    "                # Predict\n",
    "                with torch.no_grad():\n",
    "                    pred = net(input_tensor)\n",
    "                    pred = torch.softmax(pred[0], dim=0)\n",
    "                    seg = torch.argmax(pred, dim=0).numpy()\n",
    "\n",
    "                # print(f\"Processed patient {pid}, slice {idx}: seg shape {seg.shape}, dtype {seg.dtype}\")\n",
    "                # segs.append(seg)\n",
    "\n",
    "                processed_imgs_stack[:, :, idx] = img  # Store the processed image for this slice\n",
    "                seg_stack[:, :, idx] = seg  # Store the segmentation for this slice\n",
    "\n",
    "                # # Save\n",
    "                # pid_folder = os.path.join(output_root, str(pid))\n",
    "                # os.makedirs(pid_folder, exist_ok=True)\n",
    "\n",
    "                # affine = np.eye(4)  # identity affine \n",
    "                # #TODO: Look into this. This is an identity affine to map from numpy array to nifti file format, but we should probably\n",
    "                # # use the one from the DICOM - or does this not matter for input into Giulia's code?\n",
    "\n",
    "                # nib.save(nib.Nifti1Image(img, affine), os.path.join(pid_folder, f\"{idx}_img.nii.gz\"))\n",
    "                # nib.save(nib.Nifti1Image(seg.astype(np.uint8), affine), os.path.join(pid_folder, f\"{idx}_seg.nii.gz\"))\n",
    "\n",
    "                # print(f\"Saved {pid} slice {idx}\")\n",
    "            # seg_stack = np.stack(segs, axis=-1)  # Stack all segmentations along the last dimension\n",
    "            \n",
    "            # Save all slices for this patient\n",
    "            pid_folder = os.path.join(output_root, str(pid))\n",
    "            os.makedirs(pid_folder, exist_ok=True)\n",
    "\n",
    "            affine = np.eye(4)  # identity affine\n",
    "            # affine = np.eye(9)\n",
    "            # affine = nifti.affine\n",
    "            \n",
    "            # Save the entire image stack and segmentation stack as NIfTI files\n",
    "            nib.save(nib.Nifti1Image(processed_imgs_stack, affine), os.path.join(pid_folder, \"img_stack.nii.gz\"))\n",
    "            nib.save(nib.Nifti1Image(seg_stack.astype(np.uint8), affine), os.path.join(pid_folder, \"seg_stack.nii.gz\"))\n",
    "\n",
    "            print(f\"Saved all slices for patient {pid} to {pid_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "44b97738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /Users/inad001/Documents/SSCP25/Data and scripts SSCP25/ED_segmentation_data/segmentation_stacks/.DS_Store is not a directory\n",
      "Saved all slices for patient 95 to /Users/inad001/Documents/SSCP25/segmented_nifti/95\n",
      "Saved all slices for patient 132 to /Users/inad001/Documents/SSCP25/segmented_nifti/132\n",
      "Saved all slices for patient 92 to /Users/inad001/Documents/SSCP25/segmented_nifti/92\n",
      "Saved all slices for patient 66 to /Users/inad001/Documents/SSCP25/segmented_nifti/66\n",
      "Saved all slices for patient 103 to /Users/inad001/Documents/SSCP25/segmented_nifti/103\n",
      "Saved all slices for patient 157 to /Users/inad001/Documents/SSCP25/segmented_nifti/157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5v/yhx8r6xn05vgqm6zxlfh68w00000gp/T/ipykernel_32526/3813278547.py:70: RuntimeWarning: invalid value encountered in divide\n",
      "  input_tensor = torch.from_numpy(img / img.max()).float()[None, None, :, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all slices for patient 150 to /Users/inad001/Documents/SSCP25/segmented_nifti/150\n",
      "Saved all slices for patient 166 to /Users/inad001/Documents/SSCP25/segmented_nifti/166\n",
      "Saved all slices for patient 35 to /Users/inad001/Documents/SSCP25/segmented_nifti/35\n",
      "Saved all slices for patient 161 to /Users/inad001/Documents/SSCP25/segmented_nifti/161\n",
      "Saved all slices for patient 102 to /Users/inad001/Documents/SSCP25/segmented_nifti/102\n",
      "Saved all slices for patient 105 to /Users/inad001/Documents/SSCP25/segmented_nifti/105\n",
      "Saved all slices for patient 58 to /Users/inad001/Documents/SSCP25/segmented_nifti/58\n",
      "Saved all slices for patient 67 to /Users/inad001/Documents/SSCP25/segmented_nifti/67\n",
      "Saved all slices for patient 93 to /Users/inad001/Documents/SSCP25/segmented_nifti/93\n",
      "Saved all slices for patient 94 to /Users/inad001/Documents/SSCP25/segmented_nifti/94\n",
      "Saved all slices for patient 160 to /Users/inad001/Documents/SSCP25/segmented_nifti/160\n",
      "Saved all slices for patient 158 to /Users/inad001/Documents/SSCP25/segmented_nifti/158\n",
      "Saved all slices for patient 167 to /Users/inad001/Documents/SSCP25/segmented_nifti/167\n",
      "Saved all slices for patient 151 to /Users/inad001/Documents/SSCP25/segmented_nifti/151\n",
      "Saved all slices for patient 169 to /Users/inad001/Documents/SSCP25/segmented_nifti/169\n",
      "Saved all slices for patient 180 to /Users/inad001/Documents/SSCP25/segmented_nifti/180\n",
      "Saved all slices for patient 187 to /Users/inad001/Documents/SSCP25/segmented_nifti/187\n",
      "Saved all slices for patient 27 to /Users/inad001/Documents/SSCP25/segmented_nifti/27\n",
      "Saved all slices for patient 145 to /Users/inad001/Documents/SSCP25/segmented_nifti/145\n",
      "Saved all slices for patient 142 to /Users/inad001/Documents/SSCP25/segmented_nifti/142\n",
      "Saved all slices for patient 129 to /Users/inad001/Documents/SSCP25/segmented_nifti/129\n",
      "Saved all slices for patient 73 to /Users/inad001/Documents/SSCP25/segmented_nifti/73\n",
      "Saved all slices for patient 118 to /Users/inad001/Documents/SSCP25/segmented_nifti/118\n",
      "Saved all slices for patient 127 to /Users/inad001/Documents/SSCP25/segmented_nifti/127\n",
      "Saved all slices for patient 120 to /Users/inad001/Documents/SSCP25/segmented_nifti/120\n",
      "Saved all slices for patient 17 to /Users/inad001/Documents/SSCP25/segmented_nifti/17\n",
      "Saved all slices for patient 188 to /Users/inad001/Documents/SSCP25/segmented_nifti/188\n",
      "Saved all slices for patient 144 to /Users/inad001/Documents/SSCP25/segmented_nifti/144\n",
      "Saved all slices for patient 1 to /Users/inad001/Documents/SSCP25/segmented_nifti/1\n",
      "Saved all slices for patient 8 to /Users/inad001/Documents/SSCP25/segmented_nifti/8\n",
      "Saved all slices for patient 181 to /Users/inad001/Documents/SSCP25/segmented_nifti/181\n",
      "Saved all slices for patient 175 to /Users/inad001/Documents/SSCP25/segmented_nifti/175\n",
      "Saved all slices for patient 21 to /Users/inad001/Documents/SSCP25/segmented_nifti/21\n",
      "Saved all slices for patient 75 to /Users/inad001/Documents/SSCP25/segmented_nifti/75\n",
      "Saved all slices for patient 121 to /Users/inad001/Documents/SSCP25/segmented_nifti/121\n",
      "Saved all slices for patient 126 to /Users/inad001/Documents/SSCP25/segmented_nifti/126\n",
      "Saved all slices for patient 162 to /Users/inad001/Documents/SSCP25/segmented_nifti/162\n",
      "Saved all slices for patient 31 to /Users/inad001/Documents/SSCP25/segmented_nifti/31\n",
      "Saved all slices for patient 91 to /Users/inad001/Documents/SSCP25/segmented_nifti/91\n",
      "Saved all slices for patient 65 to /Users/inad001/Documents/SSCP25/segmented_nifti/65\n",
      "Saved all slices for patient 62 to /Users/inad001/Documents/SSCP25/segmented_nifti/62\n",
      "Saved all slices for patient 96 to /Users/inad001/Documents/SSCP25/segmented_nifti/96\n",
      "Saved all slices for patient 109 to /Users/inad001/Documents/SSCP25/segmented_nifti/109\n",
      "Saved all slices for patient 54 to /Users/inad001/Documents/SSCP25/segmented_nifti/54\n",
      "Saved all slices for patient 107 to /Users/inad001/Documents/SSCP25/segmented_nifti/107\n",
      "Saved all slices for patient 138 to /Users/inad001/Documents/SSCP25/segmented_nifti/138\n",
      "Saved all slices for patient 53 to /Users/inad001/Documents/SSCP25/segmented_nifti/53\n",
      "Saved all slices for patient 164 to /Users/inad001/Documents/SSCP25/segmented_nifti/164\n",
      "Saved all slices for patient 163 to /Users/inad001/Documents/SSCP25/segmented_nifti/163\n",
      "Saved all slices for patient 39 to /Users/inad001/Documents/SSCP25/segmented_nifti/39\n",
      "Saved all slices for patient 152 to /Users/inad001/Documents/SSCP25/segmented_nifti/152\n",
      "Saved all slices for patient 106 to /Users/inad001/Documents/SSCP25/segmented_nifti/106\n",
      "Saved all slices for patient 139 to /Users/inad001/Documents/SSCP25/segmented_nifti/139\n",
      "Saved all slices for patient 137 to /Users/inad001/Documents/SSCP25/segmented_nifti/137\n",
      "Saved all slices for patient 97 to /Users/inad001/Documents/SSCP25/segmented_nifti/97\n",
      "Saved all slices for patient 108 to /Users/inad001/Documents/SSCP25/segmented_nifti/108\n",
      "Saved all slices for patient 63 to /Users/inad001/Documents/SSCP25/segmented_nifti/63\n",
      "Saved all slices for patient 130 to /Users/inad001/Documents/SSCP25/segmented_nifti/130\n",
      "Saved all slices for patient 112 to /Users/inad001/Documents/SSCP25/segmented_nifti/112\n",
      "Saved all slices for patient 123 to /Users/inad001/Documents/SSCP25/segmented_nifti/123\n",
      "Saved all slices for patient 70 to /Users/inad001/Documents/SSCP25/segmented_nifti/70\n",
      "Saved all slices for patient 84 to /Users/inad001/Documents/SSCP25/segmented_nifti/84\n",
      "Saved all slices for patient 170 to /Users/inad001/Documents/SSCP25/segmented_nifti/170\n",
      "Saved all slices for patient 141 to /Users/inad001/Documents/SSCP25/segmented_nifti/141\n",
      "Saved all slices for patient 4 to /Users/inad001/Documents/SSCP25/segmented_nifti/4\n",
      "Saved all slices for patient 3 to /Users/inad001/Documents/SSCP25/segmented_nifti/3\n",
      "Saved all slices for patient 146 to /Users/inad001/Documents/SSCP25/segmented_nifti/146\n",
      "Saved all slices for patient 12 to /Users/inad001/Documents/SSCP25/segmented_nifti/12\n",
      "Saved all slices for patient 76 to /Users/inad001/Documents/SSCP25/segmented_nifti/76\n",
      "Saved all slices for patient 114 to /Users/inad001/Documents/SSCP25/segmented_nifti/114\n",
      "Saved all slices for patient 78 to /Users/inad001/Documents/SSCP25/segmented_nifti/78\n",
      "Saved all slices for patient 178 to /Users/inad001/Documents/SSCP25/segmented_nifti/178\n",
      "Saved all slices for patient 182 to /Users/inad001/Documents/SSCP25/segmented_nifti/182\n",
      "Saved all slices for patient 176 to /Users/inad001/Documents/SSCP25/segmented_nifti/176\n",
      "Saved all slices for patient 149 to /Users/inad001/Documents/SSCP25/segmented_nifti/149\n",
      "Saved all slices for patient 171 to /Users/inad001/Documents/SSCP25/segmented_nifti/171\n"
     ]
    }
   ],
   "source": [
    "cmr_files = get_cmr_nifti_files(base_path)\n",
    "\n",
    "# Run segmentation on the CMR files\n",
    "run_segmentation([cmr_files], output_root=\"/Users/inad001/Documents/SSCP25/segmented_nifti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0dd7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
